{
  "cells": [
    {
      "metadata": {
        "_uuid": "8ea8e5ef1acfc633306e14ad50d41fb47f3c0509"
      },
      "cell_type": "markdown",
      "source": "# Linear discriminant analysis\n\n\n## Introduction\n\nIn this notebook we will study the Linear discriminant analysis (LDA) and implement it using pure numpy. LDA is a classification algorithm and it relies on heavy assumptions about the underlaying distribution of the points to be classified. This makes it quite restrictive, but very robust in practice and easy to implement, making it one of the most popular choices for classification problems.\n\n## The model\n\nSuppose we are given observations  $\\{(x_1,y_1),\\dots,(x_m,y_m)\\}\\subset \\mathbb{R}^n\\times F$ where $F$ is a finite set (which we will identify with $\\{0,\\dots,K-1 \\}$, where $k=\\# F$). For each class $j$, we have a prior probability $\\pi_j$ (so $\\sum_j \\pi_j = 1$) representing the probability that a randomly sampled observation comes from the class $j$. We will assume that the conditional density of $X$ for each class is Gaussian with the **same covariance matrix**, that is,\n$$\np(x \\ | \\ y = k) = \\dfrac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}\\exp(-\\dfrac{1}{2}(x-\\mu_k)^T\\Sigma^{-1}(x-\\mu_k))\n$$\nwhere $\\mu_k$ is the mean of each class and $\\Sigma$ is the covariance matrix common to all classes. Let $x\\in\\mathbb{R}^n$ be a point for which we want to predict the class $y$ it belongs to. By the Bayes theorem\n$$\n\\mathbb{P}(y = k \\ | \\ x) = \\dfrac{p(x \\ | \\ y = k)\\pi_k}{\\sum_{i=0}^{K-1}p(x \\ | \\ y = i)\\pi_i  }.\n$$\nThe Bayes discriminant rules predicts $y=k$ for the value of $k$ that maximizes $\\mathbb{P}(y = k \\ | \\ x) $. Note that the denominator is common for all classes, so in practice we maximize $p(x \\ | \\ y = k)\\pi_k$. In order to do so, denote $k^* = \\mathrm{argmax}_k\\mathbb{P}(y = k \\ | \\ x) $ and note that is equivalent to\n$$\nk^* = \\underset{k}{\\mathrm{argmax}} \\log\\left(  \\dfrac{\\mathbb{P}(y = k \\ | \\ x) }{\\mathbb{P}(y = i \\ | \\ x) } \\right)\n$$\nwhere the maximum is taken with respect to $k$ and $i$. Since we know the conditional density of each class, we can compute the above expression explicitly:\n\\begin{align*}\n\\log\\left(  \\dfrac{\\mathbb{P}(y = k \\ | \\ x) }{\\mathbb{P}(y = i \\ | \\ x) } \\right) &= \\log\\dfrac{\\pi_k}{\\pi_i} + \\log\\dfrac{p(x \\ | \\ y = k)}{p(x \\ | \\ y = i)} \\\\\n&= \\log\\dfrac{\\pi_k}{\\pi_i} +\\dfrac{1}{2}(x^T\\Sigma^{-1}\\mu_k+\\mu_k^T\\Sigma^{-1}x-\\mu_k^T\\Sigma^{-1}\\mu_k - x^T\\Sigma^{-1}\\mu_i-\\mu_i^T\\Sigma^{-1}x+\\mu_i^T\\Sigma^{-1}\\mu_i  ) \\\\\n&= \\log\\dfrac{\\pi_k}{\\pi_i} + x^T\\Sigma^{-1}(\\mu_k-\\mu_i) +\\dfrac{1}{2}( \\mu^T_i\\Sigma^{-1}\\mu_i - \\mu^T_k\\Sigma^{-1}\\mu_k )\\\\\n&= \\log\\dfrac{\\pi_k}{\\pi_i} + x^T\\Sigma^{-1}(\\mu_k-\\mu_i) - \\dfrac{1}{2}(\\mu_k+\\mu_i)^T\\Sigma^{-1}(\\mu_k-\\mu_i) \\\\\n&= \\delta_k(x) - \\delta_i(x)\n\\end{align*}\nwhere $\\delta_k(x)=\\log \\pi_k + x^T\\Sigma^{-1}\\mu_k-\\frac{1}{2}\\mu_k\\Sigma^{-1}\\mu_k$. Note then that $k^* = \\underset{k}{\\mathrm{argmax\\ }} \\delta_k(x)$. Now, in order to solve this optimization problem, we need $\\pi_k$ $\\mu_k$ and $\\Sigma$. Since we do not know in advance these values, we can estimate them using our training data. We use the following estimators:\n\n$\n\\begin{align}\n\\hat{\\pi}_k &= \\dfrac{\\#\\{ i : y_i = k\\}}{m} \\\\\n\\hat{\\mu_k}& = \\dfrac{1}{\\#\\{ i : y_i = k\\}}\\sum_{y_i=k} x_i \\\\\n\\hat{\\Sigma}&=\\dfrac{1}{m-K+1}\\sum_{k=0}^{K-1}\\sum_{y_i=k}(x_i-\\hat{\\mu}_k)(x_i-\\hat{\\mu}_k)^T\n\\end{align}\n$\n\nWith this we can fully write our prediction function: for a new point $x$, we predict the class $k*$ given by\n$$\nk^* = \\underset{k}{\\mathrm{argmax\\ }} \\delta_k(x).\n$$\nWe proceed to implement this algorithm."
    },
    {
      "metadata": {
        "_uuid": "e1a26252dfc06764394f981d6300422f0458e98f"
      },
      "cell_type": "markdown",
      "source": "## Implementation\n\nWe start by importing the usual libraries:"
    },
    {
      "metadata": {
        "_uuid": "a1d5b43f4c7b1c6167de727eb48879bdd0bf6d32"
      },
      "cell_type": "markdown",
      "source": "\\begin{align*}\n\\log\\left(  \\dfrac{\\mathbb{P}(y = k \\ | \\ x) }{\\mathbb{P}(y = i \\ | \\ x) } \\right) &= \\log\\dfrac{\\pi_k}{\\pi_i} + \\log\\dfrac{p(x \\ | \\ y = k)}{p(x \\ | \\ y = i)} \\\\\n&= \\log\\dfrac{\\pi_k}{\\pi_i} +\\dfrac{1}{2}(x^T\\Sigma^{-1}\\mu_k+\\mu_k^T\\Sigma^{-1}x-\\mu_k^T\\Sigma^{-1}\\mu_k - x^T\\Sigma^{-1}\\mu_i-\\mu_i^T\\Sigma^{-1}x+\\mu_i^T\\Sigma^{-1}\\mu_i  ) \\\\\n&= \\log\\dfrac{\\pi_k}{\\pi_i} + x^T\\Sigma^{-1}(\\mu_k-\\mu_i) +\\dfrac{1}{2}( \\mu^T_i\\Sigma^{-1}\\mu_i - \\mu^T_k\\Sigma^{-1}\\mu_k )\\\\\n&= \\log\\dfrac{\\pi_k}{\\pi_i} + x^T\\Sigma^{-1}(\\mu_k-\\mu_i) - \\dfrac{1}{2}(\\mu_k+\\mu_i)^T\\Sigma^{-1}(\\mu_k-\\mu_i) \\\\\n&= \\delta_k(x) - \\delta_i(x)\n\\end{align*}"
    },
    {
      "metadata": {
        "_uuid": "0f1c3b6fa1fee8b9da00ead33e080696746e52c5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e265911bfb713cb2204813cefc3d31d0cfa5d4a"
      },
      "cell_type": "markdown",
      "source": "We generate three clouds of points with Gaussian conditional distribution having the same covariance matrix:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "37f6304f99d6debb504ec518b7296eac33fd22f4"
      },
      "cell_type": "code",
      "source": "class_0 = 1000 #num obs from each class\nclass_1 = 1000\nclass_2 = 1000\nx0 = np.random.multivariate_normal([2, 0], [[2, .25],[.25, 1]], class_0)\nx1 = np.random.multivariate_normal([1, 4], [[2, .25],[.25, 1]], class_1)\nx2 = np.random.multivariate_normal([6, 8], [[2, .25],[.25, 1]], class_1)\n\nplt.plot(x0[:,0],x0[:,1],'x',color='red')\nplt.plot(x1[:,0],x1[:,1],'x',color='green')\nplt.plot(x2[:,0],x2[:,1],'x',color='blue')\nplt.figtext(0.5, 0.01, 'Figure 1: three Gaussian clouds of points. Red points represent class 0 green class 1 and blue class 2.', \n            wrap=True, horizontalalignment='center', fontsize=12)\nplt.axis('equal')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6282b22f2c94a725ee35570caff3d9988e27f883"
      },
      "cell_type": "markdown",
      "source": "We now generate a vector of labels for each of the points in each of the clouds:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "66ac3776c91146a04cb6bc142d7d11ba86b9aee3"
      },
      "cell_type": "code",
      "source": "y0 = np.zeros(len(x0))\ny1 = np.ones(len(x1))\ny2 = 2*np.ones(len(x2))\n\nX_train =  np.vstack((x0,x1,x2))\nY_train = np.append(np.append(y0,y1),y2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8b07af764f87449d4a140d50a350bf189a0c5714"
      },
      "cell_type": "markdown",
      "source": "We estimate the prior probabilities $\\pi_k$ of each class:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b66a18f71ba9c68c416c060bc84be6f22ebd7eab"
      },
      "cell_type": "code",
      "source": "pi_0 = sum(Y_train == 0)/len(Y_train)\npi_1 = sum(Y_train == 1)/len(Y_train)\npi_2 = sum(Y_train == 2)/len(Y_train)\nprior = [pi_0,pi_1,pi_2]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7ba3223a374cea28483ddf1ada51c1890886f669"
      },
      "cell_type": "markdown",
      "source": "Now we estimate the mean $\\mu_k$ of each class"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "654e4e936be801cbe783b2dd81303024214b5c8f"
      },
      "cell_type": "code",
      "source": "W = np.hstack((X_train,Y_train.reshape(-1,1)))\nmu_0 = np.mean(W[W[:,-1] == 0],axis=0)[:-1]\nmu_1 = np.mean(W[W[:,-1] == 1],axis=0)[:-1]\nmu_2 = np.mean(W[W[:,-1] == 2],axis=0)[:-1]\nmu = [mu_0,mu_1,mu_2]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5fd7879a82a2a25acb090cf8bf5835354c83039e"
      },
      "cell_type": "markdown",
      "source": "We now estimate the covariance matrix $\\Sigma$ and compute its inverse $\\Sigma^{-1}$:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b7b051ddfc6228eee01f143c50b6e94f5bbf9afc"
      },
      "cell_type": "code",
      "source": "sigma = 0\nfor i in range(3):\n    sigma += np.dot((X_train[W[:,-1] == i] - mu[i]).T,(X_train[W[:,-1] == i] - mu[i]))/(len(X_train)-3)\ninv_sigma = np.linalg.inv(sigma) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eb2ca823952625f429a0e102eb56103a523fcd84"
      },
      "cell_type": "markdown",
      "source": "Finally we can compute the decision function $\\delta_k(x)$:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d2c87ccb9718ff876c588abb8907d82856cd155"
      },
      "cell_type": "code",
      "source": "def delta(k,x):\n    return (np.dot(np.dot((x.reshape(-1,1)).T,inv_sigma),mu[k]) - (1/2)*np.dot(np.dot(mu[k].T,inv_sigma),mu[k]) \n            + np.log(prior[k]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "17fe33846a8b65bb67d786516240522c4afad04f"
      },
      "cell_type": "markdown",
      "source": "With this, we can make our predictions:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5b242f424939e6659bf04cbaa60c99fc145922f5"
      },
      "cell_type": "code",
      "source": "def prediction(x):\n    L = []\n    for k in range(3):\n        L.append(delta(k,x))\n    return np.argmax(L)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ba85b880e75f59fbbc577990fd57f6d80b7af060"
      },
      "cell_type": "markdown",
      "source": "Let us try this function for the points $(2,0),(1,4),(6,8)$, corresponding to the (real) means of classes $0,1,2$ respectively: "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7bb83bf7656523f191705c8aa0f5b9cc055ea07e"
      },
      "cell_type": "code",
      "source": "prediction(np.array([2,0])) , prediction(np.array([1,4])), prediction(np.array([6,8]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "19422ad5bed96c4ec4146dc954bb443478f6c7de"
      },
      "cell_type": "markdown",
      "source": "Finally we can draw the decision boundary of the predictions given by the algorithm:"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "cd003afa640b8b0c07b70abacf037ef5c5108955"
      },
      "cell_type": "code",
      "source": "x = np.linspace(-10, 15, 200)\ny = np.linspace(-7, 15, 200)\nX, Y = np.meshgrid(x, y)\nz = np.zeros(X.shape)\nZ = np.array(z)\nfor i in range(Z.shape[0]):\n    for j in range(Z.shape[1]):\n        Z[i,j] = prediction(np.array([x[j],y[i]]))\nplt.contourf(X, Y, Z, alpha=.5)\nC = plt.contour(X, Y, Z,  colors='black',zorder=4)\nplt.plot(x0[:,0],x0[:,1],'x',color='red',zorder=1)\nplt.plot(x1[:,0],x1[:,1],'x',color='green',zorder=2)\nplt.plot(x2[:,0],x2[:,1],'x',color='blue',zorder=3)\nplt.axis('equal')\nplt.figtext(0.5, 0.01, 'Figure 2: decision boundary for the LDA.', \n            wrap=True, horizontalalignment='center', fontsize=12)\nplt.show()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}