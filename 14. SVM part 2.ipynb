{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ea9ab27b57fbc7617ef9a747c0bec9b083adb44b"
      },
      "cell_type": "markdown",
      "source": "# Support vector machine, part 2: soft margins\n\n## Introduction \n\nIn the previous notebook we studied and implemented the Support vector machine algorithm for linearly separable datasets. In this notebook we study the case of non linearly separable data which can be reasonably separated by linear decision boundaries. \n\n## Non-linearly separable datasets\n\nSuppose we have a dataset $S = \\{(x^1,y^1),\\dots,(x^m,y^m) \\}\\subset \\mathbb{R}^n\\times\\{-1,1\\}$ consisting of pairs (features,class). This time we do not assume the data can be separated by a hyperplane, and thus, solutions to the optimization problem posed in the previous notebook may not exist. We recall the optimization problem in its primal form:\n\n$$\n\\begin{align*}\n\\min_{w,b}\\ &\\dfrac{1}{2}\\|w\\|^2\\\\\n\\text{subject to } &y^j(w\\cdot x^j + b) > 1 \\text{ for all } j. \\\\\n\\end{align*}\n$$\n\nHere $w$ represents the normal vector to the separating hyperplane, while $b$ represents a position vector for it. The condition $y^j(w\\cdot x^j + b) > 1 $ can be thought as forcing each point $x^j$ to be in the correct side of the hyperplane. If this condition cannot be met for all points, we will relax it by requiring $y^j(w\\cdot x^j + b) > 1 -\\epsilon_j $ each $j$, where $\\epsilon_j\\geq 0$. This can be thought that we allow points to cross the hyperplane and be in the wrong side of it, up to a certain margin, controlled individually for each point by $\\epsilon_i$. We want to allow the smallest margin of error as possible, so we penalize the size of the of the coefficients $\\epsilon_i$. Thus, we reformulate the optimization problem as \n\n$$\n\\begin{align*}\n\\min_{w,b,\\epsilon}\\ &\\dfrac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^m \\epsilon_i \\\\\n\\text{subject to } &y^j(w\\cdot x^j + b) > 1  - \\epsilon_i ,\\\\\n&\\epsilon_i \\geq 0\\text{ for all } j.\n\\end{align*}\n$$\n\nHere $C$ is a constant that gauges the interaction between the two terms to optimize. It constitutes a hyperparameter of the model and can be optimized using cross-validation methods. The Lagrangian to minimize for the dual problem is given by\n\n$$\n\\mathcal L (w,b,\\alpha,\\beta,\\epsilon) = -(\\dfrac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^m \\epsilon_i  - \\sum_{i=1}^m \\alpha_i (y^i(w\\cdot x^i + b) - (1 -\\epsilon_i))  -\\sum_{i=1}^m \\beta_i \\epsilon_i).\n$$\n\nThe [KKT](https://en.wikipedia.org/wiki/Karush–Kuhn–Tucker_conditions) for this problem are then:\n\n$$\n\\begin{align*}\n\\nabla_w \\mathcal{L} =  -(w - \\sum_{i=1}^m \\alpha_i y^i x^i) &=   0, \\\\\n\\nabla_b \\mathcal{L} =   -\\sum_{i=1}^m \\alpha_i y^i &=   0 , \\\\\n\\nabla_\\epsilon \\mathcal{L} = - (C-\\alpha_i - \\beta_i) &=0, \\\\\n\\alpha_i (y^i(w\\cdot x^i + b) - (1-\\epsilon_i) ) & = 0,\\\\\n\\beta_i \\epsilon_i &=0 , \\\\\n\\alpha_i & \\geq 0, \\\\\n\\beta_i & \\geq 0, \\\\\n\\epsilon_i & \\geq 0,\n\\end{align*}\n$$\n\nfor all $i$. Substituting the gradient equations in the Lagrangian, we obtain a new optimization problem given by\n\n$$\n\\begin{align*}\n\\min_{\\alpha} \\dfrac{1}{2} \\sum_{i,j=1}^m \\alpha_i \\alpha_ j y^i  y^j x^i\\cdot x^j -\\sum_{i=1}^m \\alpha_i \n\\end{align*}\n$$\n\nsubject to \n\n$$\n\\begin{align*}\n\\sum_{i=1}^m \\alpha_i y^i &=  0 \\\\\nC\\geq \\alpha_i & \\geq 0. \\\\\n\\end{align*}\n$$\n\nIf we solve this problem, we can recover $w$ from\n\n$$\nw = \\sum_{i=1}^m \\alpha_i y^i x^i\n$$\n\nand to recover $b$, we fix a support vector $x^s$ (that is, $0<\\alpha_s < C$) and $\\epsilon_s = 0$ (since $\\beta_s\\neq 0$) and from the condition $\\alpha_i (y^i(w\\cdot x^i + b) - (1-\\epsilon_i) )  = 0$ it follows that \n\n$$\nb = y_s^{-1} - w\\cdot x^s.\n$$\n\nThe predictions are then given by\n\n$$\n\\widehat y = \\mathrm{sign}(w\\cdot x + b).\n$$\n\nWe proceed to implement the algorithm.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bad345e3bbb070073830b35876e66e8606756d9f"
      },
      "cell_type": "markdown",
      "source": "## Implementation\n\nIn this section we implement the algorithm described above. We follow the same outline of the previous notebook."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f8a0d13fd5d6c5ad71612970ce8490fa47f02e30"
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "617875a12555609e9edfe89819d68a503fc87247"
      },
      "cell_type": "markdown",
      "source": "We generate two classes of points which are not linearly separable. We fix a seed of random numbers so we always get the same dataset:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1527939ac76db262eeebe28cf3092d647075ed7e"
      },
      "cell_type": "code",
      "source": "np.random.seed(6)\nclass_0 = 25 \nclass_1 = 25\n\nx0 = np.random.multivariate_normal([-2, 0], [[1, .15],[.15, 1]], class_0)\nx1 = np.random.multivariate_normal([0, 2], [[1, .25],[.25, 1]], class_1)\n\nplt.plot(x0[:,0],x0[:,1],'x',color='red')\nplt.plot(x1[:,0],x1[:,1],'x',color='blue')\nplt.figtext(0.5, 0.01, 'Figure 1: two Gaussian clouds of points. Red points represent class 1 and blue class 0', \n            wrap=True, horizontalalignment='center', fontsize=12)\nplt.axis('equal')\nplt.show()\n\ny0 = -np.ones(len(x0))\ny1 = np.ones(len(x1))\n\nX_train =  np.vstack((x0,x1))\ny_train = np.append(y0,y1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "96404aa419f0afcb1b393dcd175c9ffabdc243c1"
      },
      "cell_type": "markdown",
      "source": "We compute the matrix containing the inner products of the points of the dataset."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5165bdaa07a0f94d387c2d3b6fb2f5754ca7510"
      },
      "cell_type": "code",
      "source": "def inner_prods(X_train):\n    return np.dot(X_train,X_train.T)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "15af46d41d392d59fc5988e6213a6dc93034a8e7"
      },
      "cell_type": "markdown",
      "source": "With this, we can write the cost function as:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a83a7499ebe162cc407b85b26493c0d1ef9abbd"
      },
      "cell_type": "code",
      "source": "def cost(alpha): \n    return -(np.sum(alpha) - (1/2)*np.dot(np.multiply(alpha,y_train),\n                                        np.dot(inner_prods(X_train),np.multiply(alpha,y_train))))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f8c74947cc7f78563b2a6cfd2ac24b10cad9b3ac"
      },
      "cell_type": "markdown",
      "source": "We define the constraint given by the KKT conditions. This is of the form $\\sum_{i=1}^m \\alpha_i y^i = 0$."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "565e648ae4ec0623dfa1c79171af121bc753e0aa"
      },
      "cell_type": "code",
      "source": "def cons1(alpha):\n    return np.dot(alpha,y_train)\n\ncons = ({'type': 'eq', 'fun': cons1})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c329f3b3b2067cc037b09d059618db2a5831cbed"
      },
      "cell_type": "markdown",
      "source": "We also set the conditions for the Lagrange multipliers: $C\\geq\\alpha_i \\geq 0$. We set $C = 1000$ for the example:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec4c8ee6382e84eb3d572a1302eb925d04d23407"
      },
      "cell_type": "code",
      "source": "C = 1000\nbds = [(0,C) for _ in range(len(X_train))]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0fee47d9ba66f8bc42dec14848557508a9435ae5"
      },
      "cell_type": "markdown",
      "source": "Now, we need an initial guess for the value of $\\alpha_i$ for the optimizer:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "23cb29a47e86344313477bf65fd09dcd746791a5"
      },
      "cell_type": "code",
      "source": "alpha = np.random.random(len(X_train))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "196bbeb5cd0b41db7f4f6059040056ae9a2ac6a4"
      },
      "cell_type": "markdown",
      "source": "Finally, we optimize the cost function "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "58420d8706d87d2695faac8d82f59da1f6352099"
      },
      "cell_type": "code",
      "source": "res = minimize(cost, alpha , bounds=bds, constraints=cons)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "692ac353c202e4de221de9388e6e9261a492ba48"
      },
      "cell_type": "markdown",
      "source": "We recover the values of $\\alpha_i$ that optimize the Lagrangian:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c6343f0598677e5eaccaa893d203cb309117a9a"
      },
      "cell_type": "code",
      "source": "alpha = res.x",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "25f31cfc13796b8a5006daa8531c1d93868a79e7"
      },
      "cell_type": "markdown",
      "source": "We will set to zero the values $\\alpha_i$ which are smaller than $10^{-7}$:"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "5001faa00b97641f2bd3c05792d5bd06cac7a4b1"
      },
      "cell_type": "code",
      "source": "alpha = alpha*(1*(alpha > 10**(-7)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "073f23d46de099555abd7e1d6a2a652dcfe25e2f"
      },
      "cell_type": "markdown",
      "source": "We can now construct the parameters $w,b$:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed71dd719b6a20e6eb2cae497d2e2d5e17cb8e81"
      },
      "cell_type": "code",
      "source": "w = 0\nfor i in range(len(X_train)):\n    w += y_train[i]*alpha[i]*X_train[i,:]\n\nb = y_train[(alpha > 0) & (alpha < C)][0] - np.dot(w,X_train[(alpha > 0) & (alpha < C)][0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0d1d4676674ebce1c8abd1da0ebc30b5e9dc7f34"
      },
      "cell_type": "markdown",
      "source": "With this, we can build the prediction function:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1440c5e3027132f81c60c51c40a92db7a6a5da22"
      },
      "cell_type": "code",
      "source": "def predict(x,w,b):\n    return np.sign(np.dot(x,w)+b)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4642ebb91cbb509cc2077fe26ad2c86968e4ef5c"
      },
      "cell_type": "markdown",
      "source": "Finally, we can plot the decision boundary given by the SVM as well as the regions predicted for each class. We can see how the separating hyperplane maximizes the margin to both classes."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "98a44a029aeef73ae3b5434bcde4338df1f66e23"
      },
      "cell_type": "code",
      "source": "plt.plot(x0[:,0],x0[:,1],'x',color='red')\nplt.plot(x1[:,0],x1[:,1],'x',color='blue')\nplt.figtext(0.5, 0.01, 'Figure 2: SVM decision boundary', \n            wrap=True, horizontalalignment='center', fontsize=12)\n\nx = np.linspace(-5, 5, 200)\ny = np.linspace(-5, 5, 200)\nX, Y = np.meshgrid(x, y)\nz = np.zeros(X.shape)\nZ = np.array(z)\nfor i in range(Z.shape[0]):\n    for j in range(Z.shape[1]):\n        Z[i,j] = predict(np.array([x[j],y[i]]).reshape(1,2),w,b)\nplt.contourf(X, Y, Z, alpha=.5, cmap='jet_r')\nC = plt.contour(X, Y, Z,  colors='black',zorder=4)\nplt.axis('equal')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "835d5be8da0bf460e03d0b95a95c6b06fa6727d0"
      },
      "cell_type": "markdown",
      "source": "We can check the support vectors and plot them together with the separating hyperplane"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "282b83ac5feca00c626be63acdb19d2735fbf967"
      },
      "cell_type": "code",
      "source": "sup_vect = X_train[alpha > 0]\nsup_vect",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc5a969d0d5e50c1b52abd8c6acbf06eb9e83bbd"
      },
      "cell_type": "code",
      "source": "plt.plot(x0[:,0],x0[:,1],'x',color='red')\nplt.plot(x1[:,0],x1[:,1],'x',color='blue')\nplt.figtext(0.5, 0.01, 'Figure 3: support vectors defining the separating hyperplane', \n            wrap=True, horizontalalignment='center', fontsize=12)\n\nx = np.linspace(-5, 5, 200)\ny = np.linspace(-5, 5, 200)\nX, Y = np.meshgrid(x, y)\nplt.contourf(X, Y, Z, alpha=.5, cmap='jet_r')\nC = plt.contour(X, Y, Z,  colors='black',zorder=4)\nplt.plot(sup_vect[:,0],sup_vect[:,1],'o',color='green')\n\n\nplt.axis('equal')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3794d4d5b722d19532d5039af6861a077e3a4552"
      },
      "cell_type": "markdown",
      "source": "## Final remarks\n\nIn this notebook we implemented the support vector machine for non-linearly separable datasets, using a linear decision boundary. This meant that points from different classes were allowed to cross the (soft) margin, up to a certain distance. In the next notebook we will implement SVM with non linear decision boundaries by making use of the kernel trick."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}