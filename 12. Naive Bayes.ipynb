{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "# Naive Bayes classifier\n\n## Introduction\n\nNaive Bayes models are one of the simplest class of models for classifying tasks. It relies on heavy independence assumptions of the features, assumptions which in most cases turn out to be false, but the classifiers still perform surprisingly well, as we will see with some examples.\n\n## Model\n\nSuppose we have a dataset consisting of pairs (features,class), $\\{ (x^1,y^1),\\dots, (x^m,y^m)\\}\\subset \\mathbb{R}^m\\times F$ where $F$ is a finite set. We can identify $F$ with the set $\\{0,\\dots,K \\}$ for some positive integer number $K$. We will follow the same Bayesian approach that we folloed in previous notebooks: for each class $j$ we have a prior probability $\\pi_j$, and denote by $p_j(x \\mid y = j)$ the conditional distribution of $x=(x_1,\\dots,x_n)$ for each class. The main assumption of the model is that each function $p_j$ can be factored as a product of single variable functions:\n$$\np_j(x_1,\\dots,x_n \\mid y = j) = \\prod_{i=1}^n p_{ji}(x_i \\mid y =j)\n$$\nwhere each function $f_{ji}$ representes the conditional distribution of the $i-th$ variable given the class $j$. Thus, the Bayes theorem yields\n$\n\\begin{align*}\np(y = j \\mid x) &= \\dfrac{ p(x \\mid y = j)\\pi_j}{\\sum_{i=1}^n p(x \\mid y = i) \\pi_i} \\\\\n&= \\dfrac{\\left(\\prod_{i=1}^n p_{ji}(x_i \\mid y =j)\\right)\\pi_j}{\\sum_{i=1}^n p(x \\mid y = i)\\pi_i} \\\\\n&\\propto \\pi_j\\prod_{i=1}^n p_{ji}(x_i \\mid y =j)\n\\end{align*}\n$\nsince the denominator is the same for all classes. \n\n## Gaussian distribution\n\nFor the rest of the notebook, we will assume that each variable has a Gaussian conditional distribution, that is\n$$\np_{ji}(x_i \\mid y =j) = \\dfrac{1}{\\sqrt{2\\pi \\sigma_{ji}^2}}\\exp( -\\dfrac{(x_i - \\mu_{ji})^2}{2\\sigma_{ji}^2} ),\n$$\nwhere $\\mu_{j} = (\\mu_{ji})_{i=1}^n$ is the vector of means of each variable conditional to the class $j$ and $\\sigma_{j} = (\\sigma_{ji})_{i=1}^n$ is the vector of variances of each variable conditional to the class $j$. Note that this would correspond to the diagonal od the covariance matrix of $(x_1,\\dots,x_n)$ conditional to the class $j$ in the multivariate Gaussian distribution case (see LDA and QDA).\n\n## Maximum likelihood estimators\n\nIn previous notebooks we have used maximum likelihood estimates for the parameters of the distributions being modelled. In this section we are going to prove that these estimators indeed maximize the likelihood.\nThe main idea is that we find the parameters $\\hat\\mu,\\hat\\sigma$ of the distribution that maximize the probability of having observed that data (technically, since the distribution is continuous, they maximize the density). That is\n\n$$\n\\hat\\mu,\\hat\\sigma = \\underset{\\mu,\\sigma}{\\mathrm{argmax \\ }} p(x \\mid \\mu , \\sigma).\n$$\n\nWe will analyze each class and each variable separately, as the idea is the same for all of them. Suppose the first $s$ points of the dataset belong to the class $j$. The likelihood of having observed the data $\\{(x^1_k,j),\\dots,(x^s_k,j)\\}$ is given by\n\n$$\n\\prod_{i=1}^s p(x^i_k\\mid \\mu ,\\sigma) = \\prod_{i=1}^s \\dfrac{1}{\\sqrt{2\\pi \\sigma_{jk}^2}}\\exp( -\\dfrac{(x_k^i - \\mu_{jk})^2}{2\\sigma_{jk}^2} ).\n$$\n\nNote that since the logarithm is an increasing function, finding the argmax of the likelihood is equivalent to finding the argmax of the logarithm of it.\n\n$$\n\\log \\prod_{i=1}^s p(x^i_k\\mid \\mu ,\\sigma) = s\\log\\dfrac{1}{\\sqrt{2\\pi \\sigma_{jk}^2}} - \\sum_{i=1}^ s \\dfrac{(x_k^i - \\mu_{jk})^2}{2\\sigma_{jk}^2}.\n$$\n\nTo find the maximum, we compute the gradient $\\nabla_{\\mu_{jk},\\sigma_{jk}}$ and set it equal to zero:\n\n\\begin{align*}\n\\dfrac{\\partial}{\\partial \\mu_{jk}} \\log \\prod_{i=1}^s p(x^k_k\\mid \\mu ,\\sigma)   &= - \\sum_{i=1}^ s \\dfrac{(x_k^i - \\mu_{j1})}{\\sigma_{jk}^2}  \\\\\n\\dfrac{\\partial}{\\partial \\sigma_{jk}} \\log \\prod_{i=1}^s p(x^k_k\\mid \\mu ,\\sigma) &= -\\dfrac{s}{\\sigma_{jk}} +  \\sum_{i=1}^ s \\dfrac{(x_k^i - \\mu_{jk})^2}{\\sigma_{jk}^3}\n\\end{align*}\n\nwhich implies that\n\n$\n\\begin{align*}\n\\hat\\mu_{jk} &= \\dfrac{1}{s}  \\sum_{i=1}^ s x_k^i \\\\\n\\hat\\sigma_{jk}^2& = \\dfrac{1}{s} \\sum_{i=1}^ s (x_k^i - \\hat\\mu_{jk})^2. \\\\\n\\end{align*}\n$\n\nIt is worth pointing out that this estimator for $\\sigma^2$ is biased, so a correction can be performed to obtain an unbiased estimator:\n$$\n\\hat\\sigma_{jk}^2 = \\dfrac{1}{s-1} \\sum_{i=1}^ s (x_k^i - \\hat\\mu_{jk})^2.\n$$\nIn our context this will not be very relevant. It is also possible to compute further statistical properties of these estimators, but as we will not use them here, we postpone such computations for future notebooks."
    },
    {
      "metadata": {
        "_uuid": "bf553e6516985552f4b1c95b0b0fd13da734c2d6"
      },
      "cell_type": "markdown",
      "source": "## Implementing the algorithm\n\nWe proceed now to implement the naive Bayes classifier with randomly generated data. We start by importing the usual libraries:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ee83e21ceb2749f5707755e21b3de91dc28adc8"
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6feda0a4c42f2c878a36a7704feded48dc115848"
      },
      "cell_type": "markdown",
      "source": "We generate three clouds of data given by Gaussian multivariate distributions. Note that the covariance matrices are **not** diagonal:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fd43aa8555fe48c3272b15518965db5fd72e5f16"
      },
      "cell_type": "code",
      "source": "x0 = np.random.multivariate_normal([-1, 0], [[1.35, 0.75],[0.75, 1.35]], 200)\nx1 = np.random.multivariate_normal([-5, 5], [[0.6, .25],[.25, 0.6]], 150)\nx2 = np.random.multivariate_normal([3, 3], [[1, .35],[.35, 1]], 300)\n\nplt.plot(x0[:,0],x0[:,1],'x',color='red')\nplt.plot(x1[:,0],x1[:,1],'x',color='green')\nplt.plot(x2[:,0],x2[:,1],'x',color='blue')\nplt.figtext(0.5, 0.01, 'Figure 1: three Gaussian clouds of points. Red points represent class 0, green class 1 and blue class 2', \n            wrap=True, horizontalalignment='center', fontsize=12)\nplt.axis('equal')\nplt.show()\n\ny0 = np.zeros(len(x0))\ny1 = np.ones(len(x1))\ny2 = 2*np.ones(len(x2))\n\nX_train =  np.vstack((x0,x1,x2))\nY_train = np.append(np.append(y0,y1),y2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "972db74b5eca417c15140b50f8b1643b115d6769"
      },
      "cell_type": "markdown",
      "source": "We build the functions that estimate the parameters of the model:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "69680569463e22a3adf5689bfddba240375ca2a0"
      },
      "cell_type": "code",
      "source": "def priors(X_train,Y_train):\n    P = []\n    for i in np.unique(Y_train):\n        P.append((np.sum(Y_train == i))/(len(Y_train)))\n    return P\n\ndef means(X_train,Y_train):\n    M = []\n    for i in np.unique(Y_train):\n        M.append(np.mean(X_train[Y_train == i],axis=0))\n    return M\n\ndef variances(X_train,Y_train):\n    V = []\n    for i in np.unique(Y_train):\n        V.append(np.var(X_train[Y_train == i],axis = 0))\n    return V",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7a66635d9c887584889e422c18fa2c68912e7524"
      },
      "cell_type": "markdown",
      "source": "The function that computes the conditional distribution for each class is then given by:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9ab29e011bc835a7945241e2059f04bec7834726"
      },
      "cell_type": "code",
      "source": "def likelihood(x,means,variances,k):\n    P = 1\n    for j in range(len(means[0])):\n        P = P*(1/(np.sqrt(2*np.pi*variances[k][j])))*np.exp(-((x[j]-means[k][j])**2)/(2*variances[k][j]))\n    return P",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f1709bdfc6a13845e01da42f2b2ccda25efec1fc"
      },
      "cell_type": "markdown",
      "source": "Finally, we predict the class that maximizes the product of the likelihood and the priors:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f09c7ca1a8b8dffa7ab5f2de649ab30a29ae9693"
      },
      "cell_type": "code",
      "source": "def prediction(x,means,variance,priors):\n    L = []\n    for k in range(len(means)):\n        L.append(priors[k]*likelihood(x,means,variance,k) )\n    return np.argmax(L)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "78c699b26389979538da037401340eb5cbeaec88"
      },
      "cell_type": "markdown",
      "source": "We put everything together in a class object for the classifier. Its methods are ``fit``, that trains the model (i.e., computes the values of the parameters $\\pi,\\mu,\\sigma$) and ``pred`` which makes predictions based on the current values of the parameters."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e6882e319deeae80b33ee84e02177a53f095f0e"
      },
      "cell_type": "code",
      "source": "class NaiveBayes:    \n    def __init__(self):\n        self.prior = []\n        self.means = []\n        self.covar = []\n    \n    def fit(self,X_train,Y_train):\n        self.prior = priors(X_train,Y_train)\n        self.means = means(X_train,Y_train)\n        self.var = variances(X_train,Y_train)\n    \n    def pred(self,x):\n        return prediction(x,self.means,self.var,self.prior)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "06628735127bbdf6087fc53e1604be257e4fcbb5"
      },
      "cell_type": "markdown",
      "source": "We fit the model to our training data and plot the decision boundary of the classifier:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e5ce1685811d556ffc8ef55a90178869eebeb4e0"
      },
      "cell_type": "code",
      "source": "model = NaiveBayes()\nmodel.fit(X_train,Y_train)\n\nx = np.linspace(-10, 10, 200)\ny = np.linspace(-10, 10, 200)\nX, Y = np.meshgrid(x, y)\nz = np.zeros(X.shape)\nZ = np.array(z)\nfor i in range(Z.shape[0]):\n    for j in range(Z.shape[1]):\n        Z[i,j] = model.pred(np.array([x[j],y[i]]))\nplt.contourf(X, Y, Z, alpha=.5, cmap='jet_r')\nC = plt.contour(X, Y, Z,  colors='black',zorder=4)\nplt.plot(x0[:,0],x0[:,1],'x',color='red',zorder=1)\nplt.plot(x1[:,0],x1[:,1],'x',color='green',zorder=3)\nplt.plot(x2[:,0],x2[:,1],'x',color='blue',zorder=2)\nplt.axis('equal')\nplt.figtext(0.5, 0.01, 'Figure 2: decision boundary for the naive Bayes classifier.', \n            wrap=True, horizontalalignment='center', fontsize=12)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "767c04a4f8f4d7eed90cca08f8b2f2a36c1a6b99"
      },
      "cell_type": "markdown",
      "source": "## Final remarks\n\nWe remark that the classifier is very flexible as we may use different distributions to model each conditional distribution. We also remark that this algorithm is very stable, as we will see in future notebooks. Finally, it is worth pointing out that even though the assumptions are easily violated, the fact that we do not need the **actual** probability in order to perform the predictions, so ignoring the second order terms coming from the covariance of the variables might not have a very big impact in the final predictions."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}